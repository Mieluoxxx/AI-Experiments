{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a07089c9-6c73-4cfb-9d02-bc05b00a64f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:rv19fnvu) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.012 MB of 0.012 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▂▂▃▃▃▄▄▅▅▆▆▆▇▇██</td></tr><tr><td>train_accuracy</td><td>▁▅▇▇██████████████</td></tr><tr><td>train_loss</td><td>█▄▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>valid_accuracy</td><td>▁▅▆▇██████████████</td></tr><tr><td>valid_loss</td><td>█▃▂▁▁▁▂▂▂▃▃▃▃▃▃▃▃▄</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>18</td></tr><tr><td>train_accuracy</td><td>0.9999</td></tr><tr><td>train_loss</td><td>1e-05</td></tr><tr><td>valid_accuracy</td><td>0.88388</td></tr><tr><td>valid_loss</td><td>0.01711</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">deep-rain-42</strong> at: <a href='https://wandb.ai/moguw/intent-recognition/runs/rv19fnvu' target=\"_blank\">https://wandb.ai/moguw/intent-recognition/runs/rv19fnvu</a><br/> View project at: <a href='https://wandb.ai/moguw/intent-recognition' target=\"_blank\">https://wandb.ai/moguw/intent-recognition</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240605_230529-rv19fnvu/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:rv19fnvu). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf477e63b79d45efa2221368489393cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.011112587257391877, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/root/自然语言处理/wandb/run-20240605_230627-10ptv7nk</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/moguw/intent-recognition/runs/10ptv7nk' target=\"_blank\">fiery-gorge-43</a></strong> to <a href='https://wandb.ai/moguw/intent-recognition' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/moguw/intent-recognition' target=\"_blank\">https://wandb.ai/moguw/intent-recognition</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/moguw/intent-recognition/runs/10ptv7nk' target=\"_blank\">https://wandb.ai/moguw/intent-recognition/runs/10ptv7nk</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| end of epoch   1 | time:  2.11s | train accuracy    0.497 | valid accuracy    0.648 | valid loss    0.049\n",
      "| end of epoch   2 | time:  2.17s | train accuracy    0.719 | valid accuracy    0.728 | valid loss    0.035\n",
      "| end of epoch   3 | time:  2.16s | train accuracy    0.780 | valid accuracy    0.762 | valid loss    0.028\n",
      "| end of epoch   4 | time:  2.25s | train accuracy    0.818 | valid accuracy    0.787 | valid loss    0.024\n",
      "| end of epoch   5 | time:  2.13s | train accuracy    0.849 | valid accuracy    0.803 | valid loss    0.022\n",
      "| end of epoch   6 | time:  2.24s | train accuracy    0.873 | valid accuracy    0.812 | valid loss    0.021\n",
      "| end of epoch   7 | time:  2.22s | train accuracy    0.884 | valid accuracy    0.821 | valid loss    0.020\n",
      "| end of epoch   8 | time:  2.37s | train accuracy    0.893 | valid accuracy    0.826 | valid loss    0.019\n",
      "| end of epoch   9 | time:  2.46s | train accuracy    0.903 | valid accuracy    0.832 | valid loss    0.019\n",
      "| end of epoch  10 | time:  2.61s | train accuracy    0.915 | valid accuracy    0.834 | valid loss    0.018\n",
      "| end of epoch  11 | time:  2.54s | train accuracy    0.921 | valid accuracy    0.834 | valid loss    0.018\n",
      "| end of epoch  12 | time:  2.30s | train accuracy    0.927 | valid accuracy    0.838 | valid loss    0.018\n",
      "| end of epoch  13 | time:  2.35s | train accuracy    0.931 | valid accuracy    0.838 | valid loss    0.017\n",
      "| end of epoch  14 | time:  2.26s | train accuracy    0.935 | valid accuracy    0.841 | valid loss    0.017\n",
      "| end of epoch  15 | time:  2.27s | train accuracy    0.939 | valid accuracy    0.839 | valid loss    0.017\n",
      "| end of epoch  16 | time:  2.11s | train accuracy    0.942 | valid accuracy    0.840 | valid loss    0.017\n",
      "| end of epoch  17 | time:  2.21s | train accuracy    0.945 | valid accuracy    0.842 | valid loss    0.017\n",
      "| end of epoch  18 | time:  2.37s | train accuracy    0.946 | valid accuracy    0.843 | valid loss    0.017\n",
      "| end of epoch  19 | time:  2.25s | train accuracy    0.948 | valid accuracy    0.843 | valid loss    0.017\n",
      "| end of epoch  20 | time:  2.17s | train accuracy    0.949 | valid accuracy    0.844 | valid loss    0.017\n",
      "| end of epoch  21 | time:  2.20s | train accuracy    0.951 | valid accuracy    0.845 | valid loss    0.017\n",
      "| end of epoch  22 | time:  2.19s | train accuracy    0.952 | valid accuracy    0.844 | valid loss    0.017\n",
      "| end of epoch  23 | time:  2.23s | train accuracy    0.952 | valid accuracy    0.843 | valid loss    0.016\n",
      "| end of epoch  24 | time:  2.15s | train accuracy    0.954 | valid accuracy    0.845 | valid loss    0.017\n",
      "| end of epoch  25 | time:  2.23s | train accuracy    0.953 | valid accuracy    0.845 | valid loss    0.016\n",
      "| end of epoch  26 | time:  2.22s | train accuracy    0.955 | valid accuracy    0.845 | valid loss    0.016\n",
      "| end of epoch  27 | time:  2.18s | train accuracy    0.955 | valid accuracy    0.845 | valid loss    0.016\n",
      "| end of epoch  28 | time:  2.35s | train accuracy    0.955 | valid accuracy    0.845 | valid loss    0.016\n",
      "| end of epoch  29 | time:  2.17s | train accuracy    0.956 | valid accuracy    0.845 | valid loss    0.016\n",
      "| end of epoch  30 | time:  2.21s | train accuracy    0.957 | valid accuracy    0.846 | valid loss    0.016\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.012 MB of 0.012 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇███</td></tr><tr><td>train_accuracy</td><td>▁▄▅▆▆▇▇▇▇▇▇███████████████████</td></tr><tr><td>train_loss</td><td>█▅▄▃▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>valid_accuracy</td><td>▁▄▅▆▆▇▇▇██████████████████████</td></tr><tr><td>valid_loss</td><td>█▅▄▃▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>30</td></tr><tr><td>train_accuracy</td><td>0.95651</td></tr><tr><td>train_loss</td><td>0.00728</td></tr><tr><td>valid_accuracy</td><td>0.84587</td></tr><tr><td>valid_loss</td><td>0.0163</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">fiery-gorge-43</strong> at: <a href='https://wandb.ai/moguw/intent-recognition/runs/10ptv7nk' target=\"_blank\">https://wandb.ai/moguw/intent-recognition/runs/10ptv7nk</a><br/> View project at: <a href='https://wandb.ai/moguw/intent-recognition' target=\"_blank\">https://wandb.ai/moguw/intent-recognition</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240605_230627-10ptv7nk/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import jieba\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import DataLoader\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from torch.utils.data.dataset import random_split\n",
    "from torchtext.data.functional import to_map_style_dataset\n",
    "import wandb\n",
    "from argparse import Namespace\n",
    "\n",
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "def load_data(train_file, test_file):\n",
    "    train_data = pd.read_csv(train_file, sep='\\t', header=None)\n",
    "    test_data = pd.read_csv(test_file, sep='\\t', header=None)\n",
    "    train_data[1], lbl = pd.factorize(train_data[1])\n",
    "    return train_data, test_data, lbl\n",
    "\n",
    "def coustom_data_iter(texts, labels):\n",
    "    for x, y in zip(texts, labels):\n",
    "        yield x, y\n",
    "\n",
    "def build_vocab(data_iter, tokenizer):\n",
    "    def yield_tokens(data_iter):\n",
    "        for text, _ in data_iter:\n",
    "            yield tokenizer(text)\n",
    "    vocab = build_vocab_from_iterator(yield_tokens(data_iter), specials=[\"<unk>\"])\n",
    "    vocab.set_default_index(vocab[\"<unk>\"])\n",
    "    return vocab\n",
    "\n",
    "def text_pipeline(text, vocab, tokenizer):\n",
    "    return vocab(tokenizer(text))\n",
    "\n",
    "def collate_batch(batch, vocab, tokenizer, device, max_len=40):\n",
    "    label_list, text_list = [], []\n",
    "    for (_text, _label) in batch:\n",
    "        label_list.append(_label)\n",
    "        processed_text = torch.tensor(text_pipeline(_text, vocab, tokenizer), dtype=torch.int64)\n",
    "        processed_text = F.pad(processed_text, pad=(0, max_len - len(processed_text)), mode='constant', value=0)\n",
    "        text_list.append(processed_text)\n",
    "    label_list = torch.tensor(label_list, dtype=torch.int64)\n",
    "    text_list = pad_sequence(text_list, batch_first=True)\n",
    "    return label_list.to(device), text_list.to(device)\n",
    "\n",
    "class textCNN(nn.Module):\n",
    "    def __init__(self, vocab_size, emb_dim, kernel_wins, num_class=12, dropout_rate=0):\n",
    "        super(textCNN, self).__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, emb_dim)\n",
    "        self.convs = nn.ModuleList([nn.Conv2d(1, emb_dim, (w, emb_dim)) for w in kernel_wins])\n",
    "        self.dropout = nn.Dropout(dropout_rate)  # 添加Dropout层\n",
    "        self.fc = nn.Linear(len(kernel_wins)*emb_dim, num_class)\n",
    "\n",
    "    def forward(self, x):\n",
    "        emb_x = self.embed(x)\n",
    "        emb_x = emb_x.unsqueeze(1)\n",
    "        con_x = [conv(emb_x) for conv in self.convs]\n",
    "        pool_x = [F.max_pool1d(x.squeeze(-1), x.size()[2]) for x in con_x]\n",
    "        fc_x = torch.cat(pool_x, dim=1)\n",
    "        fc_x = fc_x.squeeze(-1)\n",
    "        fc_x = self.dropout(fc_x)  # 在全连接层之前应用Dropout\n",
    "        logit = self.fc(fc_x)\n",
    "        return logit\n",
    "\n",
    "\n",
    "def train(dataloader, model, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    total_acc, total_loss, total_count = 0, 0, 0\n",
    "    for label, text in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        predicted_label = model(text)\n",
    "        loss = criterion(predicted_label, label)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.1)\n",
    "        optimizer.step()\n",
    "        total_acc += (predicted_label.argmax(1) == label).sum().item()\n",
    "        total_loss += loss.item()\n",
    "        total_count += label.size(0)\n",
    "    return total_acc / total_count, total_loss / total_count\n",
    "\n",
    "def evaluate(dataloader, model, criterion, device):\n",
    "    model.eval()\n",
    "    total_acc, total_loss, total_count = 0, 0, 0\n",
    "    with torch.no_grad():\n",
    "        for label, text in dataloader:\n",
    "            predicted_label = model(text)\n",
    "            loss = criterion(predicted_label, label)\n",
    "            total_acc += (predicted_label.argmax(1) == label).sum().item()\n",
    "            total_loss += loss.item()\n",
    "            total_count += label.size(0)\n",
    "    return total_acc / total_count, total_loss / total_count\n",
    "\n",
    "def predict(dataloader, model, device, lbl):\n",
    "    model.eval()\n",
    "    test_pred = []\n",
    "    with torch.no_grad():\n",
    "        for label, text in dataloader:\n",
    "            predicted_label = model(text).argmax(1)\n",
    "            test_pred += list(predicted_label.cpu().numpy())\n",
    "    return [lbl[x] for x in test_pred]\n",
    "\n",
    "def run(args):\n",
    "    # Initialize wandb\n",
    "    wandb.init(project=args.project_name, config=args)\n",
    "\n",
    "    # Load data\n",
    "    train_data, test_data, lbl = load_data(args.train_file, args.test_file)\n",
    "\n",
    "    # Tokenizer\n",
    "    tokenizer = jieba.lcut\n",
    "\n",
    "    # Build vocabulary\n",
    "    train_iter = coustom_data_iter(train_data[0].values[:], train_data[1].values[:])\n",
    "    vocab = build_vocab(train_iter, tokenizer)\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # Data preparation\n",
    "    train_iter = coustom_data_iter(train_data[0].values[:], train_data[1].values[:])\n",
    "    train_dataset = to_map_style_dataset(train_iter)\n",
    "\n",
    "    num_train = int(len(train_dataset) * args.split)\n",
    "    split_train_, split_valid_ = random_split(train_dataset, [num_train, len(train_dataset) - num_train])\n",
    "\n",
    "    train_dataloader = DataLoader(split_train_, batch_size=args.batch_size, shuffle=True, collate_fn=lambda batch: collate_batch(batch, vocab, tokenizer, device))\n",
    "    valid_dataloader = DataLoader(split_valid_, batch_size=args.batch_size, shuffle=True, collate_fn=lambda batch: collate_batch(batch, vocab, tokenizer, device))\n",
    "\n",
    "    # Model initialization\n",
    "    num_class = len(lbl)\n",
    "    vocab_size = len(vocab)\n",
    "    model = textCNN(vocab_size, args.embedding_dim, args.kernel_wins, num_class).to(device)\n",
    "\n",
    "    # Hyperparameters\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=args.lr)\n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=args.step_size, gamma=args.gamma)\n",
    "    best_accu = 0\n",
    "\n",
    "    # Training and evaluation\n",
    "    for epoch in range(1, args.epochs + 1):\n",
    "        epoch_start_time = time.time()\n",
    "        train_acc, train_loss = train(train_dataloader, model, criterion, optimizer, device)\n",
    "        valid_acc, valid_loss = evaluate(valid_dataloader, model, criterion, device)\n",
    "\n",
    "        wandb.log({\n",
    "            \"epoch\": epoch,\n",
    "            \"train_accuracy\": train_acc,\n",
    "            \"train_loss\": train_loss,\n",
    "            \"valid_accuracy\": valid_acc,\n",
    "            \"valid_loss\": valid_loss,\n",
    "        })\n",
    "\n",
    "        if valid_acc > best_accu:\n",
    "            best_accu = valid_acc\n",
    "            torch.save(model.state_dict(), 'TextCNN.pth')\n",
    "\n",
    "        print(f'| end of epoch {epoch:3d} | time: {time.time() - epoch_start_time:5.2f}s | '\n",
    "              f'train accuracy {train_acc:8.3f} | valid accuracy {valid_acc:8.3f} | valid loss {valid_loss:8.3f}')\n",
    "\n",
    "        scheduler.step()\n",
    "\n",
    "    # Testing\n",
    "    test_iter = coustom_data_iter(test_data[0].values[:], [0] * len(test_data))\n",
    "    test_dataset = to_map_style_dataset(test_iter)\n",
    "    test_dataloader = DataLoader(test_dataset, batch_size=args.batch_size, shuffle=False, collate_fn=lambda batch: collate_batch(batch, vocab, tokenizer, device))\n",
    "\n",
    "    test_pred = predict(test_dataloader, model, device, lbl)\n",
    "\n",
    "    pd.DataFrame({\n",
    "        'ID': range(1, len(test_pred) + 1),\n",
    "        'Target': test_pred,\n",
    "    }).to_csv('textcnn.csv', index=None)\n",
    "    wandb.finish()\n",
    "\n",
    "\n",
    "seed_everything(42)\n",
    "# Define parameters\n",
    "args = Namespace(\n",
    "    train_file='https://mirror.coggle.club/dataset/coggle-competition/intent-classify/train.csv',\n",
    "    test_file='https://mirror.coggle.club/dataset/coggle-competition/intent-classify/test.csv',\n",
    "    embedding_dim=100,\n",
    "    hidden_dim=64,\n",
    "    kernel_wins=[3, 4, 5],\n",
    "    batch_size=32,\n",
    "    epochs=30,\n",
    "    lr=0.0001,\n",
    "    step_size=5,\n",
    "    gamma=0.5,\n",
    "    split=0.8,\n",
    "    project_name='intent-recognition'\n",
    ")\n",
    "\n",
    "run(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f8e957a-5202-4985-9a14-8f1e15a35be0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aecb4dcb-adc0-4cd4-827f-f938e1dde14f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                       precision    recall  f1-score   support\n",
      "\n",
      "         Travel-Query       0.95      0.99      0.97       246\n",
      "           Music-Play       0.93      0.91      0.92       256\n",
      "        FilmTele-Play       0.86      0.93      0.90       261\n",
      "           Video-Play       0.93      0.95      0.94       264\n",
      "         Radio-Listen       0.92      0.95      0.94       244\n",
      "HomeAppliance-Control       0.96      0.97      0.96       245\n",
      "        Weather-Query       0.93      0.95      0.94       248\n",
      "         Alarm-Update       0.99      0.98      0.98       288\n",
      "       Calendar-Query       0.98      0.99      0.98       217\n",
      "       TVProgram-Play       0.88      0.60      0.71        62\n",
      "           Audio-Play       1.00      0.61      0.76        49\n",
      "                Other       0.79      0.55      0.65        40\n",
      "\n",
      "             accuracy                           0.93      2420\n",
      "            macro avg       0.93      0.86      0.89      2420\n",
      "         weighted avg       0.93      0.93      0.93      2420\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from sklearn.metrics import classification_report\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def generate_classification_report(model_path, valid_dataloader, model, lbl, device):\n",
    "    # 加载模型\n",
    "    model.load_state_dict(torch.load(model_path))\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    all_labels = []\n",
    "    all_preds = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for label, text in valid_dataloader:\n",
    "            predicted_label = model(text).argmax(1)\n",
    "            all_labels.extend(label.cpu().numpy())\n",
    "            all_preds.extend(predicted_label.cpu().numpy())\n",
    "\n",
    "    # 生成分类报告\n",
    "    report = classification_report(all_labels, all_preds, target_names=lbl)\n",
    "    print(report)\n",
    "\n",
    "# 设置参数\n",
    "model_path = 'TextCNN.pth'\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# 加载数据\n",
    "train_data, _, lbl = load_data(args.train_file, args.test_file)\n",
    "tokenizer = jieba.lcut\n",
    "vocab = build_vocab(coustom_data_iter(train_data[0].values[:], train_data[1].values[:]), tokenizer)\n",
    "\n",
    "# 数据准备\n",
    "train_iter = coustom_data_iter(train_data[0].values[:], train_data[1].values[:])\n",
    "train_dataset = to_map_style_dataset(train_iter)\n",
    "num_train = int(len(train_dataset) * args.split)\n",
    "split_train_, split_valid_ = random_split(train_dataset, [num_train, len(train_dataset) - num_train])\n",
    "\n",
    "valid_dataloader = DataLoader(split_valid_, batch_size=args.batch_size, shuffle=True, collate_fn=lambda batch: collate_batch(batch, vocab, tokenizer, device))\n",
    "\n",
    "# 定义模型\n",
    "num_class = len(lbl)\n",
    "vocab_size = len(vocab)\n",
    "model = textCNN(vocab_size, args.embedding_dim, args.kernel_wins, num_class).to(device)\n",
    "\n",
    "# 生成分类报告\n",
    "generate_classification_report(model_path, valid_dataloader, model, lbl, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "716a8824-a7b5-4548-ab2c-9f73659314ed",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
