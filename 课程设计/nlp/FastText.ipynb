{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "296e5ff4-af6a-466c-af2a-6807fd115724",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmoguw\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3dccd72d95d48b5a7c690c331746c10",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.011112448122973243, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/root/自然语言处理/wandb/run-20240605_230921-049xakb4</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/moguw/intent-recognition/runs/049xakb4' target=\"_blank\">fanciful-dew-44</a></strong> to <a href='https://wandb.ai/moguw/intent-recognition' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/moguw/intent-recognition' target=\"_blank\">https://wandb.ai/moguw/intent-recognition</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/moguw/intent-recognition/runs/049xakb4' target=\"_blank\">https://wandb.ai/moguw/intent-recognition/runs/049xakb4</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Loading model cost 0.759 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| end of epoch   1 | time:  1.95s | valid accuracy    0.655 | valid loss    1.514\n",
      "| end of epoch   2 | time:  1.68s | valid accuracy    0.760 | valid loss    1.026\n",
      "| end of epoch   3 | time:  1.56s | valid accuracy    0.802 | valid loss    0.805\n",
      "| end of epoch   4 | time:  1.60s | valid accuracy    0.833 | valid loss    0.682\n",
      "| end of epoch   5 | time:  1.57s | valid accuracy    0.842 | valid loss    0.602\n",
      "| end of epoch   6 | time:  1.74s | valid accuracy    0.857 | valid loss    0.546\n",
      "| end of epoch   7 | time:  1.56s | valid accuracy    0.864 | valid loss    0.507\n",
      "| end of epoch   8 | time:  1.49s | valid accuracy    0.868 | valid loss    0.480\n",
      "| end of epoch   9 | time:  1.65s | valid accuracy    0.874 | valid loss    0.455\n",
      "| end of epoch  10 | time:  1.56s | valid accuracy    0.879 | valid loss    0.432\n",
      "| end of epoch  11 | time:  1.63s | valid accuracy    0.882 | valid loss    0.418\n",
      "| end of epoch  12 | time:  1.65s | valid accuracy    0.886 | valid loss    0.406\n",
      "| end of epoch  13 | time:  1.51s | valid accuracy    0.887 | valid loss    0.393\n",
      "| end of epoch  14 | time:  1.57s | valid accuracy    0.891 | valid loss    0.383\n",
      "| end of epoch  15 | time:  1.55s | valid accuracy    0.890 | valid loss    0.376\n",
      "| end of epoch  16 | time:  1.53s | valid accuracy    0.895 | valid loss    0.372\n",
      "| end of epoch  17 | time:  1.51s | valid accuracy    0.895 | valid loss    0.369\n",
      "| end of epoch  18 | time:  1.51s | valid accuracy    0.895 | valid loss    0.368\n",
      "| end of epoch  19 | time:  1.66s | valid accuracy    0.895 | valid loss    0.364\n",
      "| end of epoch  20 | time:  1.55s | valid accuracy    0.897 | valid loss    0.367\n",
      "| end of epoch  21 | time:  1.52s | valid accuracy    0.895 | valid loss    0.361\n",
      "| end of epoch  22 | time:  1.49s | valid accuracy    0.896 | valid loss    0.362\n",
      "| end of epoch  23 | time:  1.54s | valid accuracy    0.896 | valid loss    0.363\n",
      "| end of epoch  24 | time:  1.56s | valid accuracy    0.897 | valid loss    0.360\n",
      "| end of epoch  25 | time:  1.68s | valid accuracy    0.897 | valid loss    0.360\n",
      "| end of epoch  26 | time:  1.71s | valid accuracy    0.897 | valid loss    0.360\n",
      "| end of epoch  27 | time:  1.57s | valid accuracy    0.897 | valid loss    0.361\n",
      "| end of epoch  28 | time:  1.83s | valid accuracy    0.897 | valid loss    0.360\n",
      "| end of epoch  29 | time:  1.52s | valid accuracy    0.897 | valid loss    0.361\n",
      "| end of epoch  30 | time:  1.59s | valid accuracy    0.897 | valid loss    0.360\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='4.270 MB of 4.270 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇███</td></tr><tr><td>train_accuracy</td><td>▁▅▆▆▆▇▇▇▇▇▇▇██████████████████</td></tr><tr><td>train_loss</td><td>█▅▄▄▃▂▂▃▂▂▂▁▂▂▁▂▂▂▂▁▁▁▁▁▁▃▁▁▁▁</td></tr><tr><td>valid_accuracy</td><td>▁▄▅▆▆▇▇▇▇▇████████████████████</td></tr><tr><td>valid_loss</td><td>█▅▄▃▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>30</td></tr><tr><td>train_accuracy</td><td>0.96023</td></tr><tr><td>train_loss</td><td>0.1046</td></tr><tr><td>valid_accuracy</td><td>0.89669</td></tr><tr><td>valid_loss</td><td>0.36012</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">fanciful-dew-44</strong> at: <a href='https://wandb.ai/moguw/intent-recognition/runs/049xakb4' target=\"_blank\">https://wandb.ai/moguw/intent-recognition/runs/049xakb4</a><br/> View project at: <a href='https://wandb.ai/moguw/intent-recognition' target=\"_blank\">https://wandb.ai/moguw/intent-recognition</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240605_230921-049xakb4/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import os\n",
    "import jieba\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import wandb\n",
    "import argparse\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import DataLoader\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from torch.utils.data.dataset import random_split\n",
    "from torchtext.data.functional import to_map_style_dataset\n",
    "\n",
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "\n",
    "def load_data(train_file, test_file):\n",
    "    train_data = pd.read_csv(train_file, sep='\\t', header=None)\n",
    "    test_data = pd.read_csv(test_file, sep='\\t', header=None)\n",
    "    train_data[1], lbl = pd.factorize(train_data[1])\n",
    "    return train_data, test_data, lbl\n",
    "\n",
    "def custom_data_iter(texts, labels):\n",
    "    for x, y in zip(texts, labels):\n",
    "        yield x, y\n",
    "\n",
    "def yield_tokens(data_iter):\n",
    "    for text, _ in data_iter:\n",
    "        yield jieba.lcut(text)\n",
    "\n",
    "def build_vocab(data_iter):\n",
    "    vocab = build_vocab_from_iterator(yield_tokens(data_iter), specials=[\"<unk>\"])\n",
    "    vocab.set_default_index(vocab[\"<unk>\"])\n",
    "    return vocab\n",
    "\n",
    "def text_pipeline(x, vocab): \n",
    "    return vocab(jieba.lcut(x))\n",
    "\n",
    "def collate_batch(batch, vocab, device):\n",
    "    label_list, text_list, offsets = [], [], [0]\n",
    "    for (_text, _label) in batch:\n",
    "        label_list.append(_label)\n",
    "        processed_text = torch.tensor(text_pipeline(_text, vocab), dtype=torch.int64)\n",
    "        text_list.append(processed_text)\n",
    "        offsets.append(processed_text.size(0))\n",
    "\n",
    "    label_list = torch.tensor(label_list, dtype=torch.int64)\n",
    "    offsets = torch.tensor(offsets[:-1]).cumsum(dim=0)\n",
    "    text_list = torch.cat(text_list)\n",
    "    return label_list.to(device), text_list.to(device), offsets.to(device)\n",
    "\n",
    "class FastText(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, num_class):\n",
    "        super(FastText, self).__init__()\n",
    "        self.embedding = nn.EmbeddingBag(vocab_size, embed_dim, sparse=False)\n",
    "        self.fc = nn.Linear(embed_dim, num_class)\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        initrange = 0.5\n",
    "        self.embedding.weight.data.uniform_(-initrange, initrange)\n",
    "        self.fc.weight.data.uniform_(-initrange, initrange)\n",
    "        self.fc.bias.data.zero_()\n",
    "\n",
    "    def forward(self, text, offsets):\n",
    "        embedded = self.embedding(text, offsets)\n",
    "        return self.fc(embedded)\n",
    "\n",
    "def train(dataloader, model, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    total_acc, total_count = 0, 0\n",
    "\n",
    "    for idx, (label, text, offsets) in enumerate(dataloader):\n",
    "        optimizer.zero_grad()\n",
    "        predicted_label = model(text, offsets)\n",
    "        loss = criterion(predicted_label, label)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.1)\n",
    "        optimizer.step()\n",
    "        total_acc += (predicted_label.argmax(1) == label).sum().item()\n",
    "        total_count += label.size(0)\n",
    "\n",
    "    return total_acc / total_count, loss.item()\n",
    "\n",
    "def evaluate(dataloader, model, criterion, device):\n",
    "    model.eval()\n",
    "    total_acc, total_count = 0, 0\n",
    "    total_loss = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for idx, (label, text, offsets) in enumerate(dataloader):\n",
    "            predicted_label = model(text, offsets)\n",
    "            loss = criterion(predicted_label, label)\n",
    "            total_acc += (predicted_label.argmax(1) == label).sum().item()\n",
    "            total_count += label.size(0)\n",
    "            total_loss += loss.item()\n",
    "    return total_acc / total_count, total_loss / len(dataloader)\n",
    "\n",
    "def predict(dataloader, model, device):\n",
    "    model.eval()\n",
    "    test_pred = []\n",
    "    with torch.no_grad():\n",
    "        for idx, (label, text, offsets) in enumerate(dataloader):\n",
    "            predicted_label = model(text, offsets).argmax(1)\n",
    "            test_pred += list(predicted_label.cpu().numpy())\n",
    "    return test_pred\n",
    "\n",
    "def run(args):\n",
    "    # Initialize wandb\n",
    "    wandb.init(project=args.project_name, config=args)\n",
    "\n",
    "    # Load data\n",
    "    train_data, test_data, lbl = load_data(args.train_file, args.test_file)\n",
    "\n",
    "    # Build vocabulary\n",
    "    train_iter = custom_data_iter(train_data[0].values[:], train_data[1].values[:])\n",
    "    vocab = build_vocab(train_iter)\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # Data preparation\n",
    "    train_iter = custom_data_iter(train_data[0].values[:], train_data[1].values[:])\n",
    "    train_dataset = to_map_style_dataset(train_iter)\n",
    "\n",
    "    num_train = int(len(train_dataset) * args.split)\n",
    "    split_train_, split_valid_ = random_split(train_dataset, [num_train, len(train_dataset) - num_train])\n",
    "\n",
    "    train_dataloader = DataLoader(split_train_, batch_size=args.batch_size, shuffle=True, collate_fn=lambda batch: collate_batch(batch, vocab, device))\n",
    "    valid_dataloader = DataLoader(split_valid_, batch_size=args.batch_size, shuffle=True, collate_fn=lambda batch: collate_batch(batch, vocab, device))\n",
    "\n",
    "    num_class = len(lbl)\n",
    "    vocab_size = len(vocab)\n",
    "    model = FastText(vocab_size, args.embedding_size, num_class).to(device)\n",
    "\n",
    "    # Hyperparameters\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=args.learning_rate)\n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=args.gamma)\n",
    "    total_accu = None\n",
    "    best_accu = 0\n",
    "\n",
    "    # Log hyperparameters with wandb\n",
    "    wandb.config.update({\n",
    "        \"epochs\": args.epochs,\n",
    "        \"learning_rate\": args.learning_rate,\n",
    "        \"batch_size\": args.batch_size,\n",
    "        \"embedding_size\": args.embedding_size\n",
    "    })\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(1, args.epochs + 1):\n",
    "        epoch_start_time = time.time()\n",
    "        train_acc, train_loss = train(train_dataloader, model, criterion, optimizer, device)\n",
    "        accu_val, val_loss = evaluate(valid_dataloader, model, criterion, device)\n",
    "\n",
    "        # Log metrics with wandb\n",
    "        wandb.log({\n",
    "            \"epoch\": epoch,\n",
    "            \"train_accuracy\": train_acc,\n",
    "            \"train_loss\": train_loss,\n",
    "            \"valid_accuracy\": accu_val,\n",
    "            \"valid_loss\": val_loss\n",
    "        })\n",
    "\n",
    "        if total_accu is not None and total_accu > accu_val:\n",
    "            scheduler.step()\n",
    "        else:\n",
    "            total_accu = accu_val\n",
    "\n",
    "        if accu_val > best_accu:\n",
    "            best_accu = accu_val\n",
    "            torch.save(model.state_dict(), args.model_path)\n",
    "            wandb.save(args.model_path)\n",
    "\n",
    "        print('| end of epoch {:3d} | time: {:5.2f}s | valid accuracy {:8.3f} | valid loss {:8.3f}'.format(epoch, time.time() - epoch_start_time, accu_val, val_loss))\n",
    "\n",
    "    # Test set predictions\n",
    "    test_iter = custom_data_iter(test_data[0].values[:], [0] * len(test_data))\n",
    "    test_dataset = to_map_style_dataset(test_iter)\n",
    "    test_dataloader = DataLoader(test_dataset, batch_size=args.batch_size, shuffle=False, collate_fn=lambda batch: collate_batch(batch, vocab, device))\n",
    "\n",
    "    test_pred = predict(test_dataloader, model, device)\n",
    "    test_pred = [lbl[x] for x in test_pred]\n",
    "\n",
    "    pd.DataFrame({\n",
    "        'ID': range(1, len(test_pred) + 1),\n",
    "        'Target': test_pred,\n",
    "    }).to_csv('fasttext.csv', index=None)\n",
    "\n",
    "    # Mark the run as finished\n",
    "    wandb.finish()\n",
    "\n",
    "# Create a Namespace object for Jupyter Notebook\n",
    "seed_everything(42)\n",
    "args = argparse.Namespace(\n",
    "    train_file='https://mirror.coggle.club/dataset/coggle-competition/intent-classify/train.csv',\n",
    "    test_file='https://mirror.coggle.club/dataset/coggle-competition/intent-classify/test.csv',\n",
    "    batch_size=32,\n",
    "    split=0.8,\n",
    "    embedding_size=100,\n",
    "    epochs=30,\n",
    "    learning_rate=1,\n",
    "    gamma=0.5,\n",
    "    model_path='FastText.pth',\n",
    "    project_name='intent-recognition',\n",
    ")\n",
    "\n",
    "# Run the training and evaluation process\n",
    "run(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab88a5e9-c3f7-462b-a13a-ea5f4c36e6a6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2c3d0f0a-550f-4092-b256-273783d734a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                       precision    recall  f1-score   support\n",
      "\n",
      "         Travel-Query       0.95      0.99      0.97       262\n",
      "           Music-Play       0.95      0.95      0.95       261\n",
      "        FilmTele-Play       0.87      0.94      0.90       249\n",
      "           Video-Play       0.96      0.94      0.95       258\n",
      "         Radio-Listen       0.95      0.96      0.95       246\n",
      "HomeAppliance-Control       0.97      0.97      0.97       252\n",
      "        Weather-Query       0.98      0.98      0.98       261\n",
      "         Alarm-Update       0.98      0.94      0.96       265\n",
      "       Calendar-Query       0.98      0.99      0.98       219\n",
      "       TVProgram-Play       0.84      0.71      0.77        52\n",
      "           Audio-Play       0.83      0.74      0.79        47\n",
      "                Other       0.73      0.62      0.67        48\n",
      "\n",
      "             accuracy                           0.95      2420\n",
      "            macro avg       0.92      0.90      0.90      2420\n",
      "         weighted avg       0.94      0.95      0.94      2420\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from sklearn.metrics import classification_report\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def generate_classification_report(model_path, valid_dataloader, model, lbl, device):\n",
    "    # 加载模型\n",
    "    model.load_state_dict(torch.load(model_path))\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    all_labels = []\n",
    "    all_preds = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for idx, (label, text, offsets) in enumerate(valid_dataloader):\n",
    "            predicted_label = model(text, offsets).argmax(1)\n",
    "            all_labels.extend(label.cpu().numpy())\n",
    "            all_preds.extend(predicted_label.cpu().numpy())\n",
    "\n",
    "    # 生成分类报告\n",
    "    report = classification_report(all_labels, all_preds, target_names=lbl)\n",
    "    print(report)\n",
    "\n",
    "# 设置参数\n",
    "model_path = 'FastText.pth'\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# 加载数据\n",
    "train_data, _, lbl = load_data(args.train_file, args.test_file)\n",
    "tokenizer = jieba.lcut\n",
    "vocab = build_vocab(custom_data_iter(train_data[0].values[:], train_data[1].values[:]))\n",
    "\n",
    "# 数据准备\n",
    "train_iter = custom_data_iter(train_data[0].values[:], train_data[1].values[:])\n",
    "train_dataset = to_map_style_dataset(train_iter)\n",
    "num_train = int(len(train_dataset) * args.split)\n",
    "split_train_, split_valid_ = random_split(train_dataset, [num_train, len(train_dataset) - num_train])\n",
    "\n",
    "valid_dataloader = DataLoader(split_valid_, batch_size=args.batch_size, shuffle=True, collate_fn=lambda batch: collate_batch(batch, vocab, device))\n",
    "\n",
    "# 定义模型\n",
    "num_class = len(lbl)\n",
    "vocab_size = len(vocab)\n",
    "model = FastText(vocab_size, args.embedding_size, num_class)\n",
    "\n",
    "# 生成分类报告\n",
    "generate_classification_report(model_path, valid_dataloader, model, lbl, device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a26a73b-f6ef-4359-90b5-d46e896d5044",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
