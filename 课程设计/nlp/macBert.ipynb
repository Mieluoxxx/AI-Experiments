{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "26fc44b9-7ada-4bc5-afaf-eedf1b954685",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import os\n",
    "\n",
    "result = subprocess.run('bash -c \"source /etc/network_turbo && env | grep proxy\"', shell=True, capture_output=True, text=True)\n",
    "output = result.stdout\n",
    "for line in output.splitlines():\n",
    "    if '=' in line:\n",
    "        var, value = line.split('=', 1)\n",
    "        os.environ[var] = value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b280f6ed-6174-49e8-897f-f49ab236fe54",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at hfl/chinese-macbert-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/root/miniconda3/lib/python3.10/site-packages/transformers/optimization.py:588: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at hfl/chinese-macbert-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Iteration: 100, Loss: 0.9321, 16.53%\n",
      "Epoch: 0, Iteration: 200, Loss: 0.4917, 33.06%\n",
      "Epoch: 0, Iteration: 300, Loss: 0.2872, 49.59%\n",
      "Epoch: 0, Iteration: 400, Loss: 0.2804, 66.12%\n",
      "Epoch: 0, Iteration: 500, Loss: 0.1549, 82.64%\n",
      "Epoch: 0, Iteration: 600, Loss: 0.6048, 99.17%\n",
      "Epoch: 0, Average training loss: 0.6682\n",
      "Accuracy: 0.9782\n",
      "Average testing loss: 0.0983\n",
      "-------------------------------\n",
      "Epoch: 1, Iteration: 100, Loss: 0.0663, 16.53%\n",
      "Epoch: 1, Iteration: 200, Loss: 0.2800, 33.06%\n",
      "Epoch: 1, Iteration: 300, Loss: 0.2356, 49.59%\n",
      "Epoch: 1, Iteration: 400, Loss: 0.0378, 66.12%\n",
      "Epoch: 1, Iteration: 500, Loss: 0.1495, 82.64%\n",
      "Epoch: 1, Iteration: 600, Loss: 0.0222, 99.17%\n",
      "Epoch: 1, Average training loss: 0.2138\n",
      "Accuracy: 0.9823\n",
      "Average testing loss: 0.0736\n",
      "-------------------------------\n",
      "Epoch: 2, Iteration: 100, Loss: 0.0078, 16.53%\n",
      "Epoch: 2, Iteration: 200, Loss: 0.2759, 33.06%\n",
      "Epoch: 2, Iteration: 300, Loss: 0.0077, 49.59%\n",
      "Epoch: 2, Iteration: 400, Loss: 0.0111, 66.12%\n",
      "Epoch: 2, Iteration: 500, Loss: 0.0194, 82.64%\n",
      "Epoch: 2, Iteration: 600, Loss: 0.0373, 99.17%\n",
      "Epoch: 2, Average training loss: 0.1381\n",
      "Accuracy: 0.9844\n",
      "Average testing loss: 0.0760\n",
      "-------------------------------\n",
      "Epoch: 3, Iteration: 100, Loss: 0.3859, 16.53%\n",
      "Epoch: 3, Iteration: 200, Loss: 0.0755, 33.06%\n",
      "Epoch: 3, Iteration: 300, Loss: 0.0257, 49.59%\n",
      "Epoch: 3, Iteration: 400, Loss: 0.0095, 66.12%\n",
      "Epoch: 3, Iteration: 500, Loss: 0.0516, 82.64%\n",
      "Epoch: 3, Iteration: 600, Loss: 0.2573, 99.17%\n",
      "Epoch: 3, Average training loss: 0.0850\n",
      "Accuracy: 0.9823\n",
      "Average testing loss: 0.0869\n",
      "-------------------------------\n",
      "Epoch: 4, Iteration: 100, Loss: 0.0462, 16.53%\n",
      "Epoch: 4, Iteration: 200, Loss: 0.0428, 33.06%\n",
      "Epoch: 4, Iteration: 300, Loss: 0.0020, 49.59%\n",
      "Epoch: 4, Iteration: 400, Loss: 0.3614, 66.12%\n",
      "Epoch: 4, Iteration: 500, Loss: 0.0054, 82.64%\n",
      "Epoch: 4, Iteration: 600, Loss: 0.2272, 99.17%\n",
      "Epoch: 4, Average training loss: 0.0583\n",
      "Accuracy: 0.9807\n",
      "Average testing loss: 0.1008\n",
      "-------------------------------\n",
      "Fold 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at hfl/chinese-macbert-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Iteration: 100, Loss: 1.0920, 16.53%\n",
      "Epoch: 0, Iteration: 200, Loss: 0.5357, 33.06%\n",
      "Epoch: 0, Iteration: 300, Loss: 0.3096, 49.59%\n",
      "Epoch: 0, Iteration: 400, Loss: 0.5786, 66.12%\n",
      "Epoch: 0, Iteration: 500, Loss: 0.1322, 82.64%\n",
      "Epoch: 0, Iteration: 600, Loss: 0.4417, 99.17%\n",
      "Epoch: 0, Average training loss: 0.6247\n",
      "Accuracy: 0.9741\n",
      "Average testing loss: 0.1037\n",
      "-------------------------------\n",
      "Epoch: 1, Iteration: 100, Loss: 0.4638, 16.53%\n",
      "Epoch: 1, Iteration: 200, Loss: 0.0842, 33.06%\n",
      "Epoch: 1, Iteration: 300, Loss: 0.4485, 49.59%\n",
      "Epoch: 1, Iteration: 400, Loss: 0.1434, 66.12%\n",
      "Epoch: 1, Iteration: 500, Loss: 0.0967, 82.64%\n",
      "Epoch: 1, Iteration: 600, Loss: 0.7682, 99.17%\n",
      "Epoch: 1, Average training loss: 0.2027\n",
      "Accuracy: 0.9770\n",
      "Average testing loss: 0.0890\n",
      "-------------------------------\n",
      "Epoch: 2, Iteration: 100, Loss: 0.1822, 16.53%\n",
      "Epoch: 2, Iteration: 200, Loss: 0.1379, 33.06%\n",
      "Epoch: 2, Iteration: 300, Loss: 0.0321, 49.59%\n",
      "Epoch: 2, Iteration: 400, Loss: 0.0104, 66.12%\n",
      "Epoch: 2, Iteration: 500, Loss: 0.0225, 82.64%\n",
      "Epoch: 2, Iteration: 600, Loss: 0.0053, 99.17%\n",
      "Epoch: 2, Average training loss: 0.1330\n",
      "Accuracy: 0.9774\n",
      "Average testing loss: 0.0992\n",
      "-------------------------------\n",
      "Epoch: 3, Iteration: 100, Loss: 0.2108, 16.53%\n",
      "Epoch: 3, Iteration: 200, Loss: 0.0031, 33.06%\n",
      "Epoch: 3, Iteration: 300, Loss: 0.3030, 49.59%\n",
      "Epoch: 3, Iteration: 400, Loss: 0.0031, 66.12%\n",
      "Epoch: 3, Iteration: 500, Loss: 0.0074, 82.64%\n",
      "Epoch: 3, Iteration: 600, Loss: 0.0320, 99.17%\n",
      "Epoch: 3, Average training loss: 0.0809\n",
      "Accuracy: 0.9753\n",
      "Average testing loss: 0.1156\n",
      "-------------------------------\n",
      "Epoch: 4, Iteration: 100, Loss: 0.0019, 16.53%\n",
      "Epoch: 4, Iteration: 200, Loss: 0.0040, 33.06%\n",
      "Epoch: 4, Iteration: 300, Loss: 0.0045, 49.59%\n",
      "Epoch: 4, Iteration: 400, Loss: 0.0028, 66.12%\n",
      "Epoch: 4, Iteration: 500, Loss: 0.3605, 82.64%\n",
      "Epoch: 4, Iteration: 600, Loss: 0.0049, 99.17%\n",
      "Epoch: 4, Average training loss: 0.0552\n",
      "Accuracy: 0.9790\n",
      "Average testing loss: 0.1078\n",
      "-------------------------------\n",
      "Fold 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at hfl/chinese-macbert-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Iteration: 100, Loss: 1.2159, 16.53%\n",
      "Epoch: 0, Iteration: 200, Loss: 0.5786, 33.06%\n",
      "Epoch: 0, Iteration: 300, Loss: 0.8468, 49.59%\n",
      "Epoch: 0, Iteration: 400, Loss: 0.4800, 66.12%\n",
      "Epoch: 0, Iteration: 500, Loss: 0.3524, 82.64%\n",
      "Epoch: 0, Iteration: 600, Loss: 0.5096, 99.17%\n",
      "Epoch: 0, Average training loss: 0.6871\n",
      "Accuracy: 0.9757\n",
      "Average testing loss: 0.1115\n",
      "-------------------------------\n",
      "Epoch: 1, Iteration: 100, Loss: 0.4830, 16.53%\n",
      "Epoch: 1, Iteration: 200, Loss: 0.1798, 33.06%\n",
      "Epoch: 1, Iteration: 300, Loss: 0.4542, 49.59%\n",
      "Epoch: 1, Iteration: 400, Loss: 0.5498, 66.12%\n",
      "Epoch: 1, Iteration: 500, Loss: 0.1848, 82.64%\n",
      "Epoch: 1, Iteration: 600, Loss: 0.0700, 99.17%\n",
      "Epoch: 1, Average training loss: 0.2179\n",
      "Accuracy: 0.9790\n",
      "Average testing loss: 0.0879\n",
      "-------------------------------\n",
      "Epoch: 2, Iteration: 100, Loss: 0.0823, 16.53%\n",
      "Epoch: 2, Iteration: 200, Loss: 0.0085, 33.06%\n",
      "Epoch: 2, Iteration: 300, Loss: 0.0553, 49.59%\n",
      "Epoch: 2, Iteration: 400, Loss: 0.0064, 66.12%\n",
      "Epoch: 2, Iteration: 500, Loss: 0.0094, 82.64%\n",
      "Epoch: 2, Iteration: 600, Loss: 0.1436, 99.17%\n",
      "Epoch: 2, Average training loss: 0.1407\n",
      "Accuracy: 0.9844\n",
      "Average testing loss: 0.0833\n",
      "-------------------------------\n",
      "Epoch: 3, Iteration: 100, Loss: 0.1011, 16.53%\n",
      "Epoch: 3, Iteration: 200, Loss: 0.0610, 33.06%\n",
      "Epoch: 3, Iteration: 300, Loss: 0.0067, 49.59%\n",
      "Epoch: 3, Iteration: 400, Loss: 0.0032, 66.12%\n",
      "Epoch: 3, Iteration: 500, Loss: 0.3648, 82.64%\n",
      "Epoch: 3, Iteration: 600, Loss: 0.1451, 99.17%\n",
      "Epoch: 3, Average training loss: 0.0889\n",
      "Accuracy: 0.9848\n",
      "Average testing loss: 0.0838\n",
      "-------------------------------\n",
      "Epoch: 4, Iteration: 100, Loss: 0.0159, 16.53%\n",
      "Epoch: 4, Iteration: 200, Loss: 0.0021, 33.06%\n",
      "Epoch: 4, Iteration: 300, Loss: 0.0552, 49.59%\n",
      "Epoch: 4, Iteration: 400, Loss: 0.0031, 66.12%\n",
      "Epoch: 4, Iteration: 500, Loss: 0.0112, 82.64%\n",
      "Epoch: 4, Iteration: 600, Loss: 0.0032, 99.17%\n",
      "Epoch: 4, Average training loss: 0.0615\n",
      "Accuracy: 0.9819\n",
      "Average testing loss: 0.1026\n",
      "-------------------------------\n",
      "Fold 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at hfl/chinese-macbert-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Iteration: 100, Loss: 1.3313, 16.53%\n",
      "Epoch: 0, Iteration: 200, Loss: 0.9005, 33.06%\n",
      "Epoch: 0, Iteration: 300, Loss: 0.1865, 49.59%\n",
      "Epoch: 0, Iteration: 400, Loss: 0.4677, 66.12%\n",
      "Epoch: 0, Iteration: 500, Loss: 0.5219, 82.64%\n",
      "Epoch: 0, Iteration: 600, Loss: 0.4911, 99.17%\n",
      "Epoch: 0, Average training loss: 0.6324\n",
      "Accuracy: 0.9400\n",
      "Average testing loss: 0.2154\n",
      "-------------------------------\n",
      "Epoch: 1, Iteration: 100, Loss: 0.1297, 16.53%\n",
      "Epoch: 1, Iteration: 200, Loss: 0.0392, 33.06%\n",
      "Epoch: 1, Iteration: 300, Loss: 0.0543, 49.59%\n",
      "Epoch: 1, Iteration: 400, Loss: 0.0194, 66.12%\n",
      "Epoch: 1, Iteration: 500, Loss: 0.0228, 82.64%\n",
      "Epoch: 1, Iteration: 600, Loss: 0.0723, 99.17%\n",
      "Epoch: 1, Average training loss: 0.1890\n",
      "Accuracy: 0.9531\n",
      "Average testing loss: 0.1839\n",
      "-------------------------------\n",
      "Epoch: 2, Iteration: 100, Loss: 0.0150, 16.53%\n",
      "Epoch: 2, Iteration: 200, Loss: 1.0649, 33.06%\n",
      "Epoch: 2, Iteration: 300, Loss: 0.0058, 49.59%\n",
      "Epoch: 2, Iteration: 400, Loss: 0.0123, 66.12%\n",
      "Epoch: 2, Iteration: 500, Loss: 0.1611, 82.64%\n",
      "Epoch: 2, Iteration: 600, Loss: 0.0046, 99.17%\n",
      "Epoch: 2, Average training loss: 0.1216\n",
      "Accuracy: 0.9511\n",
      "Average testing loss: 0.2031\n",
      "-------------------------------\n",
      "Epoch: 3, Iteration: 100, Loss: 0.2687, 16.53%\n",
      "Epoch: 3, Iteration: 200, Loss: 0.0030, 33.06%\n",
      "Epoch: 3, Iteration: 300, Loss: 0.0033, 49.59%\n",
      "Epoch: 3, Iteration: 400, Loss: 0.0034, 66.12%\n",
      "Epoch: 3, Iteration: 500, Loss: 0.0030, 82.64%\n",
      "Epoch: 3, Iteration: 600, Loss: 0.3661, 99.17%\n",
      "Epoch: 3, Average training loss: 0.0781\n",
      "Accuracy: 0.9482\n",
      "Average testing loss: 0.2447\n",
      "-------------------------------\n",
      "Epoch: 4, Iteration: 100, Loss: 0.0018, 16.53%\n",
      "Epoch: 4, Iteration: 200, Loss: 0.0051, 33.06%\n",
      "Epoch: 4, Iteration: 300, Loss: 0.0024, 49.59%\n",
      "Epoch: 4, Iteration: 400, Loss: 0.0021, 66.12%\n",
      "Epoch: 4, Iteration: 500, Loss: 0.0020, 82.64%\n",
      "Epoch: 4, Iteration: 600, Loss: 0.0013, 99.17%\n",
      "Epoch: 4, Average training loss: 0.0505\n",
      "Accuracy: 0.9531\n",
      "Average testing loss: 0.2357\n",
      "-------------------------------\n",
      "Fold 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at hfl/chinese-macbert-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Iteration: 100, Loss: 1.0555, 16.53%\n",
      "Epoch: 0, Iteration: 200, Loss: 0.5458, 33.06%\n",
      "Epoch: 0, Iteration: 300, Loss: 0.0874, 49.59%\n",
      "Epoch: 0, Iteration: 400, Loss: 0.1243, 66.12%\n",
      "Epoch: 0, Iteration: 500, Loss: 0.0220, 82.64%\n",
      "Epoch: 0, Iteration: 600, Loss: 0.2244, 99.17%\n",
      "Epoch: 0, Average training loss: 0.4817\n",
      "Accuracy: 0.7471\n",
      "Average testing loss: 0.9137\n",
      "-------------------------------\n",
      "Epoch: 1, Iteration: 100, Loss: 0.0834, 16.53%\n",
      "Epoch: 1, Iteration: 200, Loss: 0.0099, 33.06%\n",
      "Epoch: 1, Iteration: 300, Loss: 0.7936, 49.59%\n",
      "Epoch: 1, Iteration: 400, Loss: 0.0073, 66.12%\n",
      "Epoch: 1, Iteration: 500, Loss: 0.0077, 82.64%\n",
      "Epoch: 1, Iteration: 600, Loss: 0.0050, 99.17%\n",
      "Epoch: 1, Average training loss: 0.1000\n",
      "Accuracy: 0.8010\n",
      "Average testing loss: 0.7950\n",
      "-------------------------------\n",
      "Epoch: 2, Iteration: 100, Loss: 0.4277, 16.53%\n",
      "Epoch: 2, Iteration: 200, Loss: 0.0033, 33.06%\n",
      "Epoch: 2, Iteration: 300, Loss: 0.0041, 49.59%\n",
      "Epoch: 2, Iteration: 400, Loss: 0.1232, 66.12%\n",
      "Epoch: 2, Iteration: 500, Loss: 0.0031, 82.64%\n",
      "Epoch: 2, Iteration: 600, Loss: 0.0024, 99.17%\n",
      "Epoch: 2, Average training loss: 0.0564\n",
      "Accuracy: 0.8076\n",
      "Average testing loss: 0.8279\n",
      "-------------------------------\n",
      "Epoch: 3, Iteration: 100, Loss: 0.0071, 16.53%\n",
      "Epoch: 3, Iteration: 200, Loss: 0.0019, 33.06%\n",
      "Epoch: 3, Iteration: 300, Loss: 0.0027, 49.59%\n",
      "Epoch: 3, Iteration: 400, Loss: 0.4233, 66.12%\n",
      "Epoch: 3, Iteration: 500, Loss: 0.4379, 82.64%\n",
      "Epoch: 3, Iteration: 600, Loss: 0.0014, 99.17%\n",
      "Epoch: 3, Average training loss: 0.0293\n",
      "Accuracy: 0.8170\n",
      "Average testing loss: 0.9657\n",
      "-------------------------------\n",
      "Epoch: 4, Iteration: 100, Loss: 0.0110, 16.53%\n",
      "Epoch: 4, Iteration: 200, Loss: 0.0019, 33.06%\n",
      "Epoch: 4, Iteration: 300, Loss: 0.0015, 49.59%\n",
      "Epoch: 4, Iteration: 400, Loss: 0.0011, 66.12%\n",
      "Epoch: 4, Iteration: 500, Loss: 0.0009, 82.64%\n",
      "Epoch: 4, Iteration: 600, Loss: 0.0041, 99.17%\n",
      "Epoch: 4, Average training loss: 0.0228\n",
      "Accuracy: 0.8117\n",
      "Average testing loss: 1.0433\n",
      "-------------------------------\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, AdamW, get_linear_schedule_with_warmup\n",
    "\n",
    "# Load data\n",
    "data_dir = 'https://mirror.coggle.club/dataset/coggle-competition/'\n",
    "train_data = pd.read_csv(data_dir + 'intent-classify/train.csv', sep='\\t', header=None)\n",
    "test_data = pd.read_csv(data_dir + 'intent-classify/test.csv', sep='\\t', header=None)\n",
    "\n",
    "train_data[1], lbl = pd.factorize(train_data[1])\n",
    "\n",
    "# Split data\n",
    "x_train, x_test, train_label, test_label = train_test_split(\n",
    "    train_data[0].values, train_data[1].values, test_size=0.2, stratify=train_data[1].values\n",
    ")\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"hfl/chinese-macbert-base\")\n",
    "\n",
    "# Encode data\n",
    "train_encoding = tokenizer(list(x_train), truncation=True, padding=True, max_length=30)\n",
    "test_encoding = tokenizer(list(x_test), truncation=True, padding=True, max_length=30)\n",
    "\n",
    "# Dataset class\n",
    "class NewsDataset(Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(int(self.labels[idx]))\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = NewsDataset(train_encoding, train_label)\n",
    "test_dataset = NewsDataset(test_encoding, test_label)\n",
    "\n",
    "# Accuracy calculation\n",
    "def flat_accuracy(preds, labels):\n",
    "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    return np.sum(pred_flat == labels_flat) / len(labels_flat)\n",
    "\n",
    "# Load model\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"hfl/chinese-macbert-base\", num_labels=12)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Optimizer\n",
    "optim = AdamW(model.parameters(), lr=1e-5)\n",
    "\n",
    "# Training function\n",
    "def train(model, train_loader, epoch):\n",
    "    model.train()\n",
    "    total_train_loss = 0\n",
    "    iter_num = 0\n",
    "    total_iter = len(train_loader)\n",
    "    for batch in train_loader:\n",
    "        optim.zero_grad()\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs[0]\n",
    "        total_train_loss += loss.item()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optim.step()\n",
    "        iter_num += 1\n",
    "        if iter_num % 100 == 0:\n",
    "            print(f\"Epoch: {epoch}, Iteration: {iter_num}, Loss: {loss.item():.4f}, {iter_num/total_iter*100:.2f}%\")\n",
    "    print(f\"Epoch: {epoch}, Average training loss: {total_train_loss/len(train_loader):.4f}\")\n",
    "\n",
    "# Validation function\n",
    "def validation(model, val_dataloader):\n",
    "    model.eval()\n",
    "    total_eval_accuracy = 0\n",
    "    total_eval_loss = 0\n",
    "    for batch in val_dataloader:\n",
    "        with torch.no_grad():\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "            loss = outputs[0]\n",
    "            logits = outputs[1]\n",
    "            total_eval_loss += loss.item()\n",
    "            logits = logits.detach().cpu().numpy()\n",
    "            label_ids = labels.to('cpu').numpy()\n",
    "            total_eval_accuracy += flat_accuracy(logits, label_ids)\n",
    "    avg_val_accuracy = total_eval_accuracy / len(val_dataloader)\n",
    "    print(f\"Accuracy: {avg_val_accuracy:.4f}\")\n",
    "    print(f\"Average testing loss: {total_eval_loss/len(val_dataloader):.4f}\")\n",
    "    print(\"-------------------------------\")\n",
    "\n",
    "# K-Fold Cross Validation\n",
    "kf = KFold(n_splits=5)\n",
    "fold = 0\n",
    "for train_idx, val_idx in kf.split(train_data[0].values, train_data[1].values):\n",
    "    print(f\"Fold {fold}\")\n",
    "    train_text = train_data[0].iloc[train_idx]\n",
    "    val_text = train_data[0].iloc[val_idx]\n",
    "    train_label = train_data[1].iloc[train_idx].values\n",
    "    val_label = train_data[1].iloc[val_idx].values\n",
    "\n",
    "    train_encoding = tokenizer(list(train_text), truncation=True, padding=True, max_length=30)\n",
    "    val_encoding = tokenizer(list(val_text), truncation=True, padding=True, max_length=30)\n",
    "\n",
    "    train_dataset = NewsDataset(train_encoding, train_label)\n",
    "    val_dataset = NewsDataset(val_encoding, val_label)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "    val_dataloader = DataLoader(val_dataset, batch_size=16, shuffle=True)\n",
    "\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\"hfl/chinese-macbert-base\", num_labels=12)\n",
    "    model.to(device)\n",
    "\n",
    "    optim = AdamW(model.parameters(), lr=1e-5)\n",
    "    total_steps = len(train_loader) * 1\n",
    "    scheduler = get_linear_schedule_with_warmup(optim, num_warmup_steps=0, num_training_steps=total_steps)\n",
    "\n",
    "    for epoch in range(5):\n",
    "        train(model, train_loader, epoch)\n",
    "        validation(model, val_dataloader)\n",
    "\n",
    "    torch.save(model.state_dict(), f'model_{fold}.pt')\n",
    "    fold += 1\n",
    "\n",
    "# Test data encoding\n",
    "test_encoding = tokenizer(list(test_data[0]), truncation=True, padding=True, max_length=30)\n",
    "test_dataset = NewsDataset(test_encoding, [0] * len(test_data))\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=16, shuffle=False)\n",
    "\n",
    "# Prediction function\n",
    "def prediction(model, test_dataloader):\n",
    "    model.eval()\n",
    "    pred = []\n",
    "    for batch in test_dataloader:\n",
    "        with torch.no_grad():\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "            logits = outputs[1]\n",
    "            logits = logits.detach().cpu().numpy()\n",
    "            pred.append(logits)\n",
    "    return np.vstack(pred)\n",
    "\n",
    "# Model ensemble prediction\n",
    "pred = np.zeros((len(test_data), 12))\n",
    "for path in ['model_0.pt', 'model_1.pt', 'model_2.pt', 'model_3.pt', 'model_4.pt']:\n",
    "    model.load_state_dict(torch.load(path))\n",
    "    pred += prediction(model, test_dataloader)\n",
    "\n",
    "# Save predictions\n",
    "pd.DataFrame({\n",
    "    'ID': range(1, len(test_data) + 1),\n",
    "    'Target': [lbl[x] for x in pred.argmax(1)],\n",
    "}).to_csv('nlp_submit.csv', index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ee3c6cb2-510b-4a92-8010-3b09421b6b77",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at hfl/chinese-macbert-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Label_0       1.00      0.99      1.00       177\n",
      "     Label_1       0.96      0.99      0.98       237\n",
      "     Label_2       0.99      0.99      0.99       297\n",
      "     Label_3       0.93      1.00      0.96       265\n",
      "     Label_4       1.00      0.94      0.97       229\n",
      "     Label_5       1.00      1.00      1.00       174\n",
      "     Label_6       1.00      1.00      1.00       185\n",
      "     Label_7       1.00      1.00      1.00       206\n",
      "     Label_8       1.00      1.00      1.00       178\n",
      "     Label_9       0.93      0.97      0.95       153\n",
      "    Label_10       0.96      0.90      0.93       140\n",
      "    Label_11       0.99      0.92      0.95       179\n",
      "\n",
      "    accuracy                           0.98      2420\n",
      "   macro avg       0.98      0.98      0.98      2420\n",
      "weighted avg       0.98      0.98      0.98      2420\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "def generate_classification_report(model, val_dataloader, device):\n",
    "    model.eval()\n",
    "    all_labels = []\n",
    "    all_preds = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in val_dataloader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            outputs = model(input_ids, attention_mask=attention_mask)\n",
    "            logits = outputs.logits\n",
    "            preds = logits.argmax(dim=1)\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "\n",
    "    # 生成分类报告\n",
    "    lbl = ['Label_' + str(i) for i in range(12)]  # 替换成实际的标签名\n",
    "    report = classification_report(all_labels, all_preds, target_names=lbl)\n",
    "    print(report)\n",
    "\n",
    "# 加载模型和验证集数据\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"hfl/chinese-macbert-base\", num_labels=12)\n",
    "model.load_state_dict(torch.load('model_0.pt'))  # 替换成实际的模型路径\n",
    "model.to(device)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=16, shuffle=False)  # 替换成实际的验证集数据加载器\n",
    "\n",
    "# 生成分类报告\n",
    "generate_classification_report(model, val_dataloader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52cf3336-0d7c-4208-a9b0-51bc0d2a4f49",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
